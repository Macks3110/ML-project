%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out
                                                          % if you need a4paper
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4
                                                          % paper

\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document



% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed
\usepackage[]{algorithm2e} %for pseudocode

\title{\LARGE \bf
CS433 Machine Learning Project 1 
}

\author{Francesco Bardi, Samuel Edler von Baussnern, Zeya Yin % stops a space
}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}
The goal of this project is to predict the presence of the Higgs Boson with distinguishing of the two events. There are six basic algorithms and some modifications, after enough steps of feature engineering to be implemented.
The processes to choose a simple and understandable model to be sufficient to classify the data are shown.\\


\section{Pre-Processing}
\subsection{Pre-Feacture selection}
In order to remove redundant information，the correlations between each feature is calculated and highly correlated ones are removed when the correlation exceeds a fixed threshold $c$, set to $0.995$. 

\subsection{Data Processing}
The next step is to explore the details of data set before selecting model and doing feature engineering. It is found that some values in several features are undefined when specific values are given by categorical variable \texttt{PRE\_jet\_num}. This suggests to transform the jetNUM
into a new class jetCLASS by merging 2 and 3 (either =0, 1, $>$1). We then prepare two different methods that, in first method, jet classes are encoded with new dummy variables and we just delete the jet column to make model easy . In another method, data are split according to JETCLASS num, hence each of JETCLASS will be fit with a different model that means we have multimodal. The first method is notated as method I and second is named as method II.
We expect the first method to be faster and better.

At next step, we replace the undefined values with zero for special Columns, which values are undefined given by the jetClass. In addition,
We simply drop the columns which have too many undefined values according to the given specific jet class.
%REPLACE 999 with mean().
Even after transforming there may be some NULL values as -999, which  will  impact
prediction  results  due  to  this  fake  information  with  large value. The solution is to replace them with the mean value of the corresponding feature.

\subsection{Normalization}
The common standardization is then implemented that the data is subtracted by the mean and divided by the standard deviation after the feature expansion process as demonstrated in section III.\\ 

\subsection{Oversampling}
Since the two class of data are not balanced, which can cause some misclassification issues. Hence oversampling is implemented to overcome this issue that we randomly copy some data from class which is shortage of data . Then this class will be augmented with this duplicated data.

\subsection{Naive test}
The six basic implemented algorithms are used to fit the data. 
The obtained accuracy results both for the training set and the test set are reported in Table \ref{table:test}, where we give also the parameters used for each model. 

%%\begin{center}

\begin{table}[h!]
\scriptsize
\begin{tabular}{| c | c | c | c | c | c | c |}
	%\rowcolor{gray!50}
      \hline
      Model          & Train & Test & $\lambda$ & $\gamma$ & Max Iter  \\
      \hline 
      Least squares GD   & 5.5  &  199     & 1e-3 &  6.71         & 16.72     \\
      \hline
      Least squares SGD     & 4.5  &  191     & 13    &  5.2          & 13      \\
      \hline 
      Least squares & 5    &  128     & \textbackslash   &  \textbackslash          & \textbackslash        \\
      \hline
      Ridge regression  & 3.5  &  191     & 7.43  &  \textbackslash        & \textbackslash    \\
      \hline
      Logistic regression & 3.4  &  116     & \textbackslash  &  21.04        & 4.14      \\
      \hline
      Regularized logistic regression & 3.4  &  116     & 1.88  &  21.04  & 4.14     \\
      \hline  
\end{tabular} 
\caption{Mandatory function results}
\label{table:test}
\end{table}
%%\end{center}

\section{FEATURE ENGINEERING}

\subsection{Feature Expansion}
To fit the nonlinear curve of the data set, some additional expansion basis features should be created.The basis expansion functions we used in this project are following:\\

Bias:\\
One column consist of 1 is added to data set to work as a bias term to fit the data.\\

Power:\\
The new features are created by using the basis function  $\phi(x) = x^i$,    $ \forall i$, $1<i\leq6$.\\  

Combinatory:\\
Each two features are chosen to be multiplied as $\phi(x) = x_i*x_j$, $ \forall i,j$, $i<j.$
Also another set of features are created as the absolute difference as  $\phi($x_i$,$x_j$) = \|$x_i$ - $x_j$\|$, $\forall i,j$, $i<j$.\\

tanh:\\
  $\phi(x) = tanh(x)$ are calculated so we can map x in (-1,1). This is implemented for every feature for six times.\\
  
log:\\
The log function is defined as $\phi(x) =log(x + 1 + 1000)$.\\

\subsection{Feature Selection}
Among the various methods to perform feature selection $l_1$ regularization techniques have the advantage that the selection process is part of the learning algorithm since they can converge to a sparse weight matrix.
The most appropriate learning algorithm for classification problems is the logistic regression due to its robustness to leverage points. However the large number of features generated with the expansion process make the regularized logistic computationally very expensive, moreover due to its simplicity in the implementation the LASSO regression was preferred.
The LASSO equation can be solved by means of the coordinate descent method, which solves a one dimensional minimization problem at each iteration. The Lasso shooting algorithm is very appealing since it relies on an analytic expression for the minimization step(see Algorithm \ref{shooting}).   

\begin{algorithm}[]
 \KwData{$\mathbf{X}$, $\mathbf{y}$ }
 \KwResult{optimal weights $\mathbf{w}$}
 -Initialize $w$ with RIDGE regression \\
 \While{$convergence$}{
 \For{j in 1,...,D}{
  $a_j = 2 \sum_{i=1}^n x_{ij}^2$\;
  $c_j = 2 \sum_{i=1}^n x_{ij}(y_i-\mathbf{w}^T\mathbf{x_i}+w_jx_{ij})$\;
  $w_j = soft(\frac{c_j}{a_j}, \frac{\lambda_{LASSO}}{a_j})$\;
  }
 }
 \label{shooting}
 \caption{Lasso shooting algorithm}
\end{algorithm}

The choice of the right regularization parameter $\lambda_{LASSO}$ is a critical step since it is sensitive to data perturbation; the common practice is to perform cross validation however because of time... we did the hyperpameter search directly on the splitted data set. %Please check previous sentence
After selecting a subset of features, the model weights are recomputed through RIDGE regression since the lasso regularization may over-penalize important features.

%Since the logistic regression is difficult to converge with a lot of variables, which will take more computation time. Also, it requires to do hyperparamter search on step size and regularization parameter thus the even it gives better results we still decide to use linear classifier. In order to avoid over fitting at same time, we decided to implement the regularization method.
%However, the single implemented lasso regularization may over-penalize important feature, while, the ridge method can still have some overfitting. In order to overcome these two issues, after feature expansion, the first step of feature selection is still checking the correlation, then the lasso shooting algorithm is implemented to perform feature selections  since it can converge to a sparse weight matrix. Following this, the ridge method can be used to train model. \\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Results}
Following the process as stated in section II and section III, the two methods jetnum split and single model with dummy variable are trained with lasso shooting as feature selection and  ridge regression. The hyperparameter search for both the algorithm  is reported in Table \ref{table:hyper}， where we used grid search instead of cross validation as we assume the data set is sufficiently used.
%The table shows paramters and results from lasso extraction and ridge regression

\begin{table}[h!]
\scriptsize
\begin{tabular}{| c | c | c | c | c | c | c |}
	%\rowcolor{gray!50}
      \hline
      Method          & Train & Test & Feature No &$\lambda_{LASSO}$ & $\lambda_{RIDGE}$  \\
      \hline 
      Single model   & 5.5  &  199     & 1e-3 &  6.71         & 16.72     \\
      \hline
      Multi model   & 4.5  &  191     & 13    &  5.2          & 13      \\
      \hline 
\end{tabular} 
\caption{Optimal models}
\label{table:hyper}
\end{table}


\begin{figure}[thpb]
      \centering
      \framebox{\parbox{3in}{We suggest that you use a text box to insert a graphic (which is ideally a 300 dpi TIFF or EPS file, with all fonts embedded) because, in an document, this method is somewhat more stable than directly inserting a picture.
}}
      %\includegraphics[scale=1.0]{figurefile}
      \caption{Inductance of oscillation winding on amorphous
       magnetic core versus DC bias magnetic field}
      \label{figurelabel}
   \end{figure}
The combinations of lasso feature selection and ridge  regression offers a fast training computation, easy implementation and significant improved results.And two method gives a quite close results.
\section{Discussion}

\section{Reference}


\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\end{document}
